{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from more_itertools import chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mntrain = MNIST('../samples')\n",
    "\n",
    "img_tr,lbl_tr = mntrain.load_training()\n",
    "img_ts,lbl_ts = mntrain.load_testing()\n",
    "img_size = 28 # 28x28 pixels\n",
    "\n",
    "def display_image(img):\n",
    "    plt.imshow(np.array(img).reshape(img_size,img_size), cmap=plt.cm.binary)\n",
    "    plt.gcf().axes[0].set_axis_off()\n",
    "    plt.show()\n",
    "\n",
    "display_image(img_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x, d=False, fn = 'sigmoid'):\n",
    "    \"\"\"Activation function for the neural network.\n",
    "    Args:\n",
    "        x (np.array): Input to the activation function.\n",
    "        d (bool): If True, return the derivative of the activation function.\n",
    "        fn (str): The activation function to use. Options are 'sigmoid', 'tanh', 'relu', 'leaky_relu', 'softmax', and 'linear'.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: The output of the activation function.\n",
    "    \"\"\"\n",
    "    if fn == 'sigmoid':\n",
    "        if d:\n",
    "            return x * (1 - x)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    if fn == 'tanh':\n",
    "        if d:\n",
    "            return 1 - np.tanh(x)**2\n",
    "        return np.tanh(x)\n",
    "    if fn == 'relu':\n",
    "        if d:\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        return np.maximum(0, x)\n",
    "    if fn == 'leaky_relu':\n",
    "        if d:\n",
    "            return np.where(x > 0, 1, 0.01)\n",
    "        return np.maximum(0.01*x, x)\n",
    "    if fn == 'softmax':\n",
    "        if d:\n",
    "            return x * (1 - x)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    if fn == 'linear':\n",
    "        if d:\n",
    "            return 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"A layer of neuron nodes.\n",
    "\n",
    "    Attributes:\n",
    "        length (int): The number of nodes in the layer.\n",
    "        nodes (np.array): The nodes in the layer.\n",
    "        values (np.array): The values of the nodes in the layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "        self.nodes = np.zeros(length)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Layer: length: %d\" % (self.length)\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    def __getitem__(self, index):\n",
    "        return self.nodes[index]\n",
    "    def __setitem__(self, index, value):\n",
    "        self.nodes[index].value = value\n",
    "    def __iter__(self):\n",
    "        return iter(self.nodes)\n",
    "    def __len__(self):\n",
    "        return len(self.nodes)\n",
    "    def __contains__(self, item):\n",
    "        return item in self.nodes\n",
    "    \n",
    "    def get_values(self):\n",
    "        return self.nodes\n",
    "    def set_values(self, values):\n",
    "        self.nodes = np.array(values)\n",
    "    values = property(get_values, set_values)\n",
    "\n",
    "    \n",
    "class Neurons:\n",
    "    \"\"\"A connection between two layers of neuron nodes.\n",
    "    \n",
    "    Attributes:\n",
    "        input (Layer): The input layer.\n",
    "        output (Layer): The output layer.\n",
    "        weights (np.array): The weights of the connections.\n",
    "        biases (np.array): The biases of the connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, input: Layer, output: Layer):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.weights = np.random.rand(len(output), len(input))*2-1\n",
    "        self.biases = np.random.rand(len(output))*2-1\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Layer_Connection: layer1: %s, layer2: %s, weights: %s, biases: %s\" % (len(self.input), len(self.output), self.weights.shape, self.biases.shape)\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    def __getitem__(self, index):\n",
    "        return self.weights[index]\n",
    "    def __setitem__(self, index, value):\n",
    "        self.weights[index] = value\n",
    "    def __iter__(self):\n",
    "        return iter(self.weights)\n",
    "    def __len__(self):\n",
    "        return self.weights.shape[0] * self.weights.shape[1]\n",
    "    def __contains__(self, layer):\n",
    "        return layer in (self.input, self.output)\n",
    "\n",
    "\n",
    "class Neural_Network:\n",
    "    \"\"\"A neural network.\n",
    "    \n",
    "    Attributes:\n",
    "        input_size (int): The number of inputs to the network.\n",
    "        output_size (int): The number of outputs from the network.\n",
    "        hidden_size (int): The number of nodes in each hidden layer.\n",
    "        hidden_layers (int): The number of hidden layers.\n",
    "        layers (list): The layers of the network.\n",
    "        neurons (list): The connections between the layers of the network.\n",
    "        \n",
    "    Methods:\n",
    "        feed_forward (np.array): Feed forward the inputs through the network.\n",
    "        back_propagate (np.array): Back propagate the errors through the network.\n",
    "        train (np.array): Train the network.\n",
    "        test (np.array): Test the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, output_size,hidden_size, hidden_layers=1):\n",
    "        self.input_size = 28*28 # 28x28 pixels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = [Layer(self.input_size)]\n",
    "        self.layers.extend([Layer(hidden_size) for _ in range(hidden_layers)])\n",
    "        self.layers.append(Layer(output_size))\n",
    "        \n",
    "        self.neurons = [Neurons(self.layers[i], self.layers[i+1]) for i in range(len(self.layers)-1)][::-1]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Neural_Network: input_size: %d, hidden_size: %d, output_size: %d, hidden_layers: %d, layers: %s, connections: %s\" % (self.input_size, self.hidden_size, self.output_size, len(self.layers)-2, self.layers, self.neurons)\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "        \n",
    "    def output_vector(self):\n",
    "        return self.layers[-1].values\n",
    "    \n",
    "    def output(self):\n",
    "        return np.argmax(self.output_vector())\n",
    "    \n",
    "    def feed_forward(self, image, fn='sigmoid'):\n",
    "        \"\"\"Feed forward the inputs through the network.\n",
    "        \n",
    "        Args:\n",
    "            image (np.array): The image to feed forward.\n",
    "            fn (str): The activation function to use.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.layers[0].values = np.array(image)/256 #normalize pixel values\n",
    "        neurs = self.neurons[::-1]\n",
    "        for depth, layer in enumerate(self.layers[1:]):\n",
    "            layer.values = activation(np.dot(neurs[depth].weights, self.layers[depth]) + neurs[depth].biases, fn=fn)\n",
    "            \n",
    "\n",
    "    def back_propagate(self, images, labels, factor, fn='sigmoid'):\n",
    "        \"\"\"Back propagate the errors through the network as a batch.\n",
    "        \n",
    "        Args:\n",
    "            images (np.array): The images to train on.\n",
    "            labels (np.array): The labels of the images.\n",
    "            factor (float): The factor to multiply the gradients by.\n",
    "            fn (str): The activation function to use.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        label_arrays = [np.zeros(self.output_size) for _ in range(len(labels))]\n",
    "        for i,label in enumerate(labels): #convert labels to arrays\n",
    "            label_arrays[i][label] = 1\n",
    "            \n",
    "        total_nabla_w = [np.zeros(neuron.weights.shape) for neuron in self.neurons] #initialize gradients\n",
    "        total_nabla_b = [np.zeros(neuron.biases.shape) for neuron in self.neurons] #initialize biases\n",
    "        for i in range(len(images)): #calculate gradients\n",
    "            nabla_w, nabla_b = self.back_propagate_single(images[i], label_arrays[i], fn=fn)\n",
    "            for j in range(len(nabla_w)):\n",
    "                total_nabla_w[j] += nabla_w[j]\n",
    "                total_nabla_b[j] += nabla_b[j]\n",
    "            \n",
    "        for i in range(len(self.neurons)): #update weights and biases\n",
    "            self.neurons[i].weights += factor * total_nabla_w[i]/len(images)\n",
    "            self.neurons[i].biases += factor * total_nabla_b[i]/len(images)\n",
    "        \n",
    "    def back_propagate_single(self, image, label, fn='sigmoid'):\n",
    "        \"\"\"Back propagation helper function for the errors through the network for a single image.\n",
    "        \n",
    "        Args:\n",
    "            image (np.array): The image to train on.\n",
    "            label (np.array): The label of the image.\n",
    "            fn (str): The activation function to use.\n",
    "        \n",
    "        Returns:\n",
    "            nabla_w (list): The gradients of the weights.\n",
    "            nabla_b (list): The gradients of the biases.\n",
    "        \"\"\"\n",
    "        self.feed_forward(image, fn=fn)\n",
    "        nabla_w = []\n",
    "        nabla_b = []\n",
    "        for i in range(len(self.neurons)):\n",
    "            neuron = self.neurons[i]\n",
    "            if i == 0: #output layer \n",
    "                delta = (np.array(neuron.output.values) - label) * activation(np.array(neuron.output.values), fn=fn, d=True)\n",
    "            else: #hidden layers\n",
    "                delta = np.matmul(self.neurons[i-1].weights.T, delta) * activation(np.array(neuron.output.values), fn=fn, d=True)\n",
    "            delta_w = np.outer(delta, neuron.input.values)\n",
    "            nabla_w.append(delta_w)\n",
    "            nabla_b.append(delta)\n",
    "        return nabla_w, nabla_b\n",
    "\n",
    "\n",
    "    def test(self, images, labels, fn='sigmoid'):\n",
    "        \"\"\"Test the network.\n",
    "        \n",
    "        Args:\n",
    "            images (np.array): The images to test on.\n",
    "            labels (np.array): The labels of the images.\n",
    "            fn (str): The activation function to use.\n",
    "        \n",
    "        Returns:\n",
    "            correct/len(images) (float): The accuracy of the network.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        for i in trange(len(images)):\n",
    "            self.feed_forward(images[i], fn=fn)\n",
    "            if self.output() == labels[i]:\n",
    "                correct += 1\n",
    "                print(\"Correct: \" % (correct, labels[i], self.output()))\n",
    "        return correct/len(images)\n",
    "\n",
    "              \n",
    "    def train(self, images, labels, epochs=1, factor=0.1,batch_size=1, fn='sigmoid'):\n",
    "        \"\"\"Train the network.\n",
    "        \n",
    "        Args:\n",
    "            images (np.array): The images to train on.\n",
    "            labels (np.array): The labels of the images.\n",
    "            epochs (int): The number of epochs to train for.\n",
    "            factor (float): The factor to multiply the gradients by.\n",
    "            batch_size (int): The size of the batches to train on.\n",
    "            fn (str): The activation function to use.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        img_lbls = list(zip(images,labels))\n",
    "        for _ in trange(epochs):\n",
    "            np.random.shuffle(img_lbls) #shuffle the images\n",
    "            batches = chunked(img_lbls, batch_size) #split into batches\n",
    "            for batch in tqdm(batches, leave=False, desc=\"Epoch\", total = len(images)//batch_size): #train on batches\n",
    "                batch_images = [image for image,_ in batch]\n",
    "                batch_labels = [label for _,label in batch]\n",
    "                self.back_propagate(batch_images, batch_labels, factor,fn=fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(784, 10, 16, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [14:10<24:21, 23.20s/it]"
     ]
    }
   ],
   "source": [
    "nn.train(img_tr,lbl_tr, 50, 0.2, 1, fn='relu')\n",
    "nn.train(img_tr,lbl_tr, 50, 0.3, 100, fn='relu')\n",
    "nn.train(img_tr,lbl_tr, 50, 0.4, 1000, fn='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-83f7f5dcece4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_ts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlbl_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "nn.test(img_ts,lbl_ts, fn='relu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
